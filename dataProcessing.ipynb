{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Liabraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.utils import resample\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for the Excel file stored in Google Drive\n",
    "# file_path = 'DataSet_EU_3k_5k.xlsx'\n",
    "file_path = 'Test.xlsx'\n",
    "\n",
    "# Use pandas to read the Excel file located at the specified file path\n",
    "data = pd.read_excel(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GSNR_35</th>\n",
       "      <th>GSNR_36</th>\n",
       "      <th>GSNR_37</th>\n",
       "      <th>GSNR_38</th>\n",
       "      <th>GSNR_39</th>\n",
       "      <th>GSNR_40</th>\n",
       "      <th>GSNR_41</th>\n",
       "      <th>GSNR_42</th>\n",
       "      <th>GSNR_43</th>\n",
       "      <th>GSNR_44</th>\n",
       "      <th>...</th>\n",
       "      <th>GSNR_69</th>\n",
       "      <th>GSNR_70</th>\n",
       "      <th>GSNR_71</th>\n",
       "      <th>GSNR_72</th>\n",
       "      <th>GSNR_73</th>\n",
       "      <th>GSNR_74</th>\n",
       "      <th>GSNR_75</th>\n",
       "      <th>GSNR_76</th>\n",
       "      <th>No. Spans</th>\n",
       "      <th>Total Distance(m)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77.575997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>83.989788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>86.062347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>90.061284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.045789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>690608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78.279541</td>\n",
       "      <td>80.219954</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>92.560867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.544975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>690608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>72.294523</td>\n",
       "      <td>73.59654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.952966</td>\n",
       "      <td>68.330300</td>\n",
       "      <td>65.326872</td>\n",
       "      <td>67.614572</td>\n",
       "      <td>71.283118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.991977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85.512399</td>\n",
       "      <td>83.114113</td>\n",
       "      <td>8</td>\n",
       "      <td>690608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>88.056368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>86.757766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>76.409583</td>\n",
       "      <td>73.664915</td>\n",
       "      <td>74.651700</td>\n",
       "      <td>79.071217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>690608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>83.209197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>77.030461</td>\n",
       "      <td>74.502742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>74.902228</td>\n",
       "      <td>71.293391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78.299438</td>\n",
       "      <td>74.910765</td>\n",
       "      <td>72.030907</td>\n",
       "      <td>73.790264</td>\n",
       "      <td>81.190439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>690608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GSNR_35    GSNR_36   GSNR_37    GSNR_38    GSNR_39    GSNR_40    GSNR_41  \\\n",
       "0  77.575997   0.000000   0.00000  83.989788   0.000000  86.062347   0.000000   \n",
       "1  78.279541  80.219954   0.00000   0.000000   0.000000   0.000000   0.000000   \n",
       "2   0.000000  72.294523  73.59654   0.000000   0.000000  70.952966  68.330300   \n",
       "3   0.000000   0.000000   0.00000   0.000000  88.056368   0.000000  86.757766   \n",
       "4   0.000000   0.000000   0.00000  83.209197   0.000000  77.030461  74.502742   \n",
       "\n",
       "     GSNR_42    GSNR_43    GSNR_44  ...    GSNR_69    GSNR_70    GSNR_71  \\\n",
       "0   0.000000   0.000000   0.000000  ...  90.061284   0.000000   0.000000   \n",
       "1   0.000000   0.000000   0.000000  ...   0.000000  92.560867   0.000000   \n",
       "2  65.326872  67.614572  71.283118  ...   0.000000   0.000000  90.991977   \n",
       "3   0.000000   0.000000   0.000000  ...  76.409583  73.664915  74.651700   \n",
       "4   0.000000  74.902228  71.293391  ...   0.000000   0.000000  78.299438   \n",
       "\n",
       "     GSNR_72    GSNR_73    GSNR_74    GSNR_75    GSNR_76  No. Spans  \\\n",
       "0   0.000000   0.000000  95.045789   0.000000   0.000000          8   \n",
       "1   0.000000  89.544975   0.000000   0.000000   0.000000          8   \n",
       "2   0.000000   0.000000   0.000000  85.512399  83.114113          8   \n",
       "3  79.071217   0.000000   0.000000   0.000000   0.000000          8   \n",
       "4  74.910765  72.030907  73.790264  81.190439   0.000000          8   \n",
       "\n",
       "   Total Distance(m)  \n",
       "0             690608  \n",
       "1             690608  \n",
       "2             690608  \n",
       "3             690608  \n",
       "4             690608  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataframe to get an overview of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the attribute columns and the target variable\n",
    "attribute_columns = [f'Power_{i}' for i in range(1, 77)] + [f'ASE_{i}' for i in range(1, 77)] + [f'NLI_{i}' for i in range(1, 77)]+ ['No. Spans'] + ['Total Distance(m)']\n",
    "\n",
    "# Label\n",
    "target_column = 'GSNR_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rafia\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_array_api.py:701: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "C:\\Users\\rafia\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_array_api.py:718: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'GSNR_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'GSNR_1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 12\u001b[0m\n\u001b[0;32m      7\u001b[0m x \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(x, columns\u001b[38;5;241m=\u001b[39mattribute_columns)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Extract the target variable values and reshape them for normalization\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# y = data[target_column].values.reshape(-1, 1)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Normalize the target variable using MinMaxScaler and convert the result back to a DataFrame\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m y \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     13\u001b[0m y \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(y, columns\u001b[38;5;241m=\u001b[39m[target_column])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Display the first few rows of the normalized data for selected columns\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'GSNR_1'"
     ]
    }
   ],
   "source": [
    "# MinMax Normalization\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize the attribute columns using MinMaxScaler and convert the result back to a DataFrame\n",
    "x = scaler.fit_transform(data[attribute_columns])\n",
    "x = pd.DataFrame(x, columns=attribute_columns)\n",
    "\n",
    "# Extract the target variable values and reshape them for normalization\n",
    "# y = data[target_column].values.reshape(-1, 1)\n",
    "# Normalize the target variable using MinMaxScaler and convert the result back to a DataFrame\n",
    "y = scaler.fit_transform(data[target_column])\n",
    "y = pd.DataFrame(y, columns=[target_column])\n",
    "\n",
    "# Display the first few rows of the normalized data for selected columns\n",
    "print(\"MinMax Normalized Data:\")\n",
    "print(x['Power_1'].head(), x['ASE_1'].head(), x['NLI_1'].head(), x['No. Spans'].head(), x['Total Distance(m)'].head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized data with equal path distribution in train and test data :\n",
      "\n",
      "Train data shape: (15000, 230)\n",
      "Test data shape: (3000, 230)\n",
      "Train labels shape: (15000,)\n",
      "Test labels shape: (3000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Define the function to split data according to the specified pattern\n",
    "def custom_train_test_split(data, labels, samples_per_block=3000, train_samples_per_block=2500, test_samples_per_block=500):\n",
    "    # Initialize lists to hold the training and testing data and labels\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    train_labels = []\n",
    "    test_labels = []\n",
    "\n",
    "    # Determine the total number of samples in the data\n",
    "    total_samples = len(data)\n",
    "\n",
    "    # Loop over the data in blocks of the specified size (samples_per_block)\n",
    "    for start in range(0, total_samples, samples_per_block):\n",
    "        # Define the end of the current block\n",
    "        end = start + samples_per_block\n",
    "        # Define the end of the training samples within the current block\n",
    "        train_end = start + train_samples_per_block\n",
    "\n",
    "        # Adjust train_end and end if they exceed the total number of samples\n",
    "        if train_end > total_samples:\n",
    "            train_end = total_samples\n",
    "        if end > total_samples:\n",
    "            end = total_samples\n",
    "\n",
    "        # Append the training and testing data and labels for the current block to the respective lists\n",
    "        train_data.append(data[start:train_end])\n",
    "        test_data.append(data[train_end:end])\n",
    "        train_labels.append(labels[start:train_end])\n",
    "        test_labels.append(labels[train_end:end])\n",
    "\n",
    "    # Concatenate the lists into DataFrames and reset the index\n",
    "    train_data = pd.concat(train_data).reset_index(drop=True)\n",
    "    test_data = pd.concat(test_data).reset_index(drop=True)\n",
    "    train_labels = pd.concat(train_labels).reset_index(drop=True)\n",
    "    test_labels = pd.concat(test_labels).reset_index(drop=True)\n",
    "\n",
    "    # Return the training and testing data and labels\n",
    "    return train_data, test_data, train_labels, test_labels\n",
    "\n",
    "# Create a list of attribute columns excluding those that start with 'frequency'\n",
    "# attribute_columns_without_frequency = [col for col in attribute_columns if not col.startswith('frequency')]\n",
    "\n",
    "# Assuming 'x' and 'y' are your DataFrames for MinMax normalized data\n",
    "# Perform the custom train-test split on the normalized data\n",
    "# X_train, X_test, Y_train, Y_test = custom_train_test_split(x[attribute_columns_without_frequency], y[target_column])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    x[attribute_columns], \n",
    "    y[target_column], \n",
    "    test_size=0.2,  # Equivalent to test_samples_per_block / samples_per_block\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "\n",
    "# Display shapes to verify the split\n",
    "print(\"Normalized data with equal path distribution in train and test data :\\n\")\n",
    "print(\"Train data shape:\", X_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "print(\"Train labels shape:\", Y_train.shape)\n",
    "print(\"Test labels shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the data to a file\n",
    "with open('data.pkl', 'wb') as f:\n",
    "    pickle.dump((X_train, Y_train, X_test, Y_test), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
