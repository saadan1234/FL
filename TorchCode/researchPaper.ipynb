{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Define convolutional layers with specified input/output channels, kernel size, stride, and padding\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        # Define fully connected (linear) layers\n",
    "        self.fc1 = nn.Linear(2048, 128)  # Input size depends on the output from the convolutional layers\n",
    "        self.fc2 = nn.Linear(128, 10)    # Output size corresponds to the number of classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the convolutional layers with ReLU activations\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        # Flatten the tensor from 4D to 2D, keeping the batch dimension intact\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Forward pass through the fully connected layers with ReLU activation\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        # Apply log softmax to output logits for classification\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Function to update the client's model using local training data\n",
    "def client_update(client_model, optimizer, train_loader, epoch=5):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for e in range(epoch):  # Loop over the specified number of epochs\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):  # Iterate over batches of data\n",
    "            data, target = data, target  # No need to move to CUDA as we're using CPU\n",
    "            optimizer.zero_grad()  # Zero the gradients of the optimizer\n",
    "            output = client_model(data)  # Forward pass through the model\n",
    "            loss = F.nll_loss(output, target)  # Calculate negative log likelihood loss\n",
    "            loss.backward()  # Backpropagate the loss\n",
    "            optimizer.step()  # Update the model parameters\n",
    "    return loss.item()  # Return the final loss value\n",
    "\n",
    "\n",
    "# Function to aggregate the models from all clients into the global model\n",
    "def server_aggregate(global_model, client_models):\n",
    "    global_dict = global_model.state_dict()  # Get the state dictionary of the global model\n",
    "    # Iterate over each parameter key in the global model\n",
    "    for k in global_dict.keys():\n",
    "        # Stack the parameter values from all client models and compute their mean\n",
    "        global_dict[k] = torch.stack([client_models[i].state_dict()[k] for i in range(len(client_models))], 0).mean(0)\n",
    "    global_model.load_state_dict(global_dict)  # Load the averaged parameters into the global model\n",
    "    # Update each client model with the new global model state\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "\n",
    "# Function to evaluate the global model on the test data\n",
    "def test(global_model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for data, target in test_loader:  # Iterate over the test data\n",
    "            data, target = data, target  # No need to move to CUDA as we're using CPU\n",
    "            output = global_model(data)  # Forward pass through the model\n",
    "            # Accumulate the total loss across all test batches\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            # Get the index of the max log-probability (predicted class)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            # Count the number of correct predictions\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Average loss over the dataset\n",
    "    acc = correct / len(test_loader.dataset)  # Calculate accuracy\n",
    "\n",
    "    return test_loss, acc  # Return the test loss and accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th round\n",
      "average train loss 0.486 | test loss 0.439 | test acc: 0.880\n",
      "1-th round\n",
      "average train loss 0.151 | test loss 0.253 | test acc: 0.924\n",
      "2-th round\n",
      "average train loss 0.126 | test loss 0.205 | test acc: 0.943\n",
      "3-th round\n",
      "average train loss 0.0292 | test loss 0.175 | test acc: 0.953\n",
      "4-th round\n",
      "average train loss 0.0113 | test loss 0.157 | test acc: 0.956\n"
     ]
    }
   ],
   "source": [
    "# IID (Independent and Identically Distributed) case: \n",
    "# All clients have images of all classes, simulating a balanced dataset distribution.\n",
    "\n",
    "# Hyperparameters\n",
    "num_clients = 100  # Total number of clients\n",
    "num_selected = 10  # Number of clients selected per round\n",
    "num_rounds = 5     # Number of federated learning rounds\n",
    "epochs = 5         # Number of local training epochs for each client\n",
    "batch_size = 32    # Batch size for training\n",
    "\n",
    "# Creating decentralized datasets\n",
    "# Load the MNIST training dataset and apply transformations (convert to tensor and normalize)\n",
    "traindata = datasets.MNIST('./data', train=True, download=True,\n",
    "                       transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "                       )\n",
    "# Split the training data evenly among the clients\n",
    "traindata_split = torch.utils.data.random_split(traindata, \n",
    "                      [int(traindata.data.shape[0] / num_clients) for _ in range(num_clients)])\n",
    "\n",
    "# Create data loaders for each client's data\n",
    "train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
    "\n",
    "# Create a data loader for the test set (used for global model evaluation)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=False, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        ), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate models and optimizers\n",
    "# Define the global model that will aggregate updates from client models\n",
    "global_model = Net()  # No .cuda() since you're using CPU\n",
    "# Create a list of client models, initialized to the same state as the global model\n",
    "client_models = [Net() for _ in range(num_selected)]  # All models stay on CPU\n",
    "\n",
    "# Load the initial state of the global model into each client model\n",
    "for model in client_models:\n",
    "    model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "# Define optimizers for each client model, using Stochastic Gradient Descent (SGD) with a learning rate of 0.1\n",
    "opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]\n",
    "\n",
    "# Running Federated Learning (FL)\n",
    "for r in range(num_rounds):  # Iterate over the number of federated learning rounds\n",
    "    # Select random clients for this round\n",
    "    client_idx = np.random.permutation(num_clients)[:num_selected]  # Randomly permute client indices and select\n",
    "\n",
    "    # Client update phase\n",
    "    loss = 0  # Initialize loss accumulator for averaging later\n",
    "    for i in range(num_selected):  # Iterate over the selected clients\n",
    "        # Update each selected client model with its respective optimizer and data loader\n",
    "        loss += client_update(client_models[i], opt[i], train_loader[client_idx[i]], epoch=epochs)\n",
    "    \n",
    "    # Server aggregation phase\n",
    "    # Aggregate updates from all selected client models into the global model\n",
    "    server_aggregate(global_model, client_models)\n",
    "    # Test the aggregated global model on the test data\n",
    "    test_loss, acc = test(global_model, test_loader)\n",
    "    \n",
    "    # Print the results of the current round\n",
    "    print('%d-th round' % r)\n",
    "    print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_selected, test_loss, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th round\n",
      "average train loss 0.0141 | test loss 5.76 | test acc: 0.197\n",
      "1-th round\n",
      "average train loss 0.0489 | test loss 2.77 | test acc: 0.198\n",
      "2-th round\n",
      "average train loss 0.000319 | test loss 4.26 | test acc: 0.198\n",
      "3-th round\n",
      "average train loss 0.094 | test loss 1.86 | test acc: 0.268\n",
      "4-th round\n",
      "average train loss 0.0109 | test loss 4.86 | test acc: 0.185\n",
      "5-th round\n",
      "average train loss 0.00609 | test loss 1.2 | test acc: 0.750\n",
      "6-th round\n",
      "average train loss 0.00885 | test loss 1.17 | test acc: 0.621\n"
     ]
    }
   ],
   "source": [
    "# NON-IID (Non-Independent and Identically Distributed) case: \n",
    "# Each client has images of only two categories from the pairs [0, 1], [2, 3], [4, 5], [6, 7], or [8, 9].\n",
    "\n",
    "# Hyperparameters\n",
    "num_clients = 100  # Total number of clients\n",
    "num_selected = 5   # Number of clients selected per round\n",
    "num_rounds = 10    # Number of federated learning rounds\n",
    "epochs = 5         # Number of local training epochs for each client\n",
    "batch_size = 32    # Batch size for training\n",
    "\n",
    "# Creating decentralized datasets\n",
    "# Load the MNIST training dataset and apply transformations (convert to tensor and normalize)\n",
    "traindata = datasets.MNIST('./data', train=True, download=True,\n",
    "                       transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "                       )\n",
    "\n",
    "# Create a tensor indicating which samples belong to each class (0 to 9)\n",
    "target_labels = torch.stack([traindata.targets == i for i in range(10)])\n",
    "\n",
    "# Split the dataset into groups where each group corresponds to two classes (e.g., [0, 1], [2, 3], etc.)\n",
    "target_labels_split = []\n",
    "for i in range(5):  # Loop over 5 pairs of classes\n",
    "    # Combine two class labels and split the combined indices into subsets for clients\n",
    "    target_labels_split += torch.split(torch.where(target_labels[(2 * i):(2 * (i + 1))].sum(0))[0], int(60000 / num_clients))\n",
    "\n",
    "# Create subsets of the training data based on the split indices for each client\n",
    "traindata_split = [torch.utils.data.Subset(traindata, tl) for tl in target_labels_split]\n",
    "\n",
    "# Create data loaders for each client's data, with shuffling enabled\n",
    "train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
    "\n",
    "# Create a data loader for the test set (used for global model evaluation)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=False, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        ), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate models and optimizers\n",
    "# Define the global model that will aggregate updates from client models\n",
    "global_model = Net()  # No .cuda() since you're using CPU\n",
    "# Create a list of client models, initialized to the same state as the global model\n",
    "client_models = [Net() for _ in range(num_selected)]\n",
    "\n",
    "# Load the initial state of the global model into each client model\n",
    "for model in client_models:\n",
    "    model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "# Define optimizers for each client model, using Stochastic Gradient Descent (SGD) with a learning rate of 0.1\n",
    "opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]\n",
    "\n",
    "# Running Federated Learning (FL)\n",
    "for r in range(num_rounds):  # Iterate over the number of federated learning rounds\n",
    "    # Select random clients for this round\n",
    "    client_idx = np.random.permutation(num_clients)[:num_selected]  # Randomly permute client indices and select\n",
    "\n",
    "    # Client update phase\n",
    "    loss = 0  # Initialize loss accumulator for averaging later\n",
    "    for i in range(num_selected):  # Iterate over the selected clients\n",
    "        # Update each selected client model with its respective optimizer and data loader\n",
    "        loss += client_update(client_models[i], opt[i], train_loader[client_idx[i]], epoch=epochs)\n",
    "    \n",
    "    # Server aggregation phase\n",
    "    # Aggregate updates from all selected client models into the global model\n",
    "    server_aggregate(global_model, client_models)\n",
    "    # Test the aggregated global model on the test data\n",
    "    test_loss, acc = test(global_model, test_loader)\n",
    "    \n",
    "    # Print the results of the current round\n",
    "    print('%d-th round' % r)\n",
    "    print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_selected, test_loss, acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
