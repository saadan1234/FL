{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Liabraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.utils import resample\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for the Excel file stored in Google Drive\n",
    "# file_path = 'DataSet_EU_3k_5k.xlsx'\n",
    "file_path = 'DataSet_EU_3k_5k.xlsx'\n",
    "\n",
    "# Use pandas to read the Excel file located at the specified file path\n",
    "data = pd.read_excel(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Power_1</th>\n",
       "      <th>Power_2</th>\n",
       "      <th>Power_3</th>\n",
       "      <th>Power_4</th>\n",
       "      <th>Power_5</th>\n",
       "      <th>Power_6</th>\n",
       "      <th>Power_7</th>\n",
       "      <th>Power_8</th>\n",
       "      <th>Power_9</th>\n",
       "      <th>Power_10</th>\n",
       "      <th>...</th>\n",
       "      <th>GSNR_69</th>\n",
       "      <th>GSNR_70</th>\n",
       "      <th>GSNR_71</th>\n",
       "      <th>GSNR_72</th>\n",
       "      <th>GSNR_73</th>\n",
       "      <th>GSNR_74</th>\n",
       "      <th>GSNR_75</th>\n",
       "      <th>GSNR_76</th>\n",
       "      <th>No. Spans</th>\n",
       "      <th>Total Distance(m)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>90.061284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.045789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>690608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>92.560867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.544975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>690608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.991977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85.512399</td>\n",
       "      <td>83.114113</td>\n",
       "      <td>8</td>\n",
       "      <td>690608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>76.409583</td>\n",
       "      <td>73.664915</td>\n",
       "      <td>74.651700</td>\n",
       "      <td>79.071217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>690608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78.299438</td>\n",
       "      <td>74.910765</td>\n",
       "      <td>72.030907</td>\n",
       "      <td>73.790264</td>\n",
       "      <td>81.190439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>690608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 382 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Power_1   Power_2   Power_3   Power_4   Power_5   Power_6   Power_7  \\\n",
       "0  0.000000  0.000000  0.000007  0.000007  0.000007  0.000000  0.000007   \n",
       "1  0.000000  0.000007  0.000000  0.000007  0.000000  0.000007  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000007  0.000000  0.000000  0.000000   \n",
       "3  0.000007  0.000007  0.000007  0.000007  0.000007  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000007  0.000000  0.000007  0.000000  0.000000   \n",
       "\n",
       "   Power_8   Power_9  Power_10  ...    GSNR_69    GSNR_70    GSNR_71  \\\n",
       "0      0.0  0.000007       0.0  ...  90.061284   0.000000   0.000000   \n",
       "1      0.0  0.000000       0.0  ...   0.000000  92.560867   0.000000   \n",
       "2      0.0  0.000007       0.0  ...   0.000000   0.000000  90.991977   \n",
       "3      0.0  0.000007       0.0  ...  76.409583  73.664915  74.651700   \n",
       "4      0.0  0.000007       0.0  ...   0.000000   0.000000  78.299438   \n",
       "\n",
       "     GSNR_72    GSNR_73    GSNR_74    GSNR_75    GSNR_76  No. Spans  \\\n",
       "0   0.000000   0.000000  95.045789   0.000000   0.000000          8   \n",
       "1   0.000000  89.544975   0.000000   0.000000   0.000000          8   \n",
       "2   0.000000   0.000000   0.000000  85.512399  83.114113          8   \n",
       "3  79.071217   0.000000   0.000000   0.000000   0.000000          8   \n",
       "4  74.910765  72.030907  73.790264  81.190439   0.000000          8   \n",
       "\n",
       "   Total Distance(m)  \n",
       "0             690608  \n",
       "1             690608  \n",
       "2             690608  \n",
       "3             690608  \n",
       "4             690608  \n",
       "\n",
       "[5 rows x 382 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataframe to get an overview of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the attribute columns and the target variable\n",
    "attribute_columns = [f'Power_{i}' for i in range(1, 77)] + [f'ASE_{i}' for i in range(1, 77)] + [f'NLI_{i}' for i in range(1, 77)]+ ['No. Spans'] + ['Total Distance(m)']\n",
    "\n",
    "# Label\n",
    "target_column = 'GSNR_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax Normalized Data:\n",
      "0    0.000000\n",
      "1    0.000000\n",
      "2    0.000000\n",
      "3    0.973215\n",
      "4    0.000000\n",
      "Name: Power_1, dtype: float64 0    1.000000\n",
      "1    0.974493\n",
      "2    0.982290\n",
      "3    0.000000\n",
      "4    0.952433\n",
      "Name: ASE_1, dtype: float64 0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "3    1.0\n",
      "4    0.0\n",
      "Name: NLI_1, dtype: float64 0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "3    0.0\n",
      "4    0.0\n",
      "Name: No. Spans, dtype: float64 0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "3    0.0\n",
      "4    0.0\n",
      "Name: Total Distance(m), dtype: float64\n",
      "     GSNR_1\n",
      "0  0.000000\n",
      "1  0.000000\n",
      "2  0.000000\n",
      "3  0.837358\n",
      "4  0.000000\n"
     ]
    }
   ],
   "source": [
    "# MinMax Normalization\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize the attribute columns using MinMaxScaler and convert the result back to a DataFrame\n",
    "x = scaler.fit_transform(data[attribute_columns])\n",
    "x = pd.DataFrame(x, columns=attribute_columns)\n",
    "\n",
    "# Extract the target variable values and reshape them for normalization\n",
    "y = data[target_column].values.reshape(-1, 1)\n",
    "# Normalize the target variable using MinMaxScaler and convert the result back to a DataFrame\n",
    "y = scaler.fit_transform(y)\n",
    "y = pd.DataFrame(y, columns=[target_column])\n",
    "\n",
    "# Display the first few rows of the normalized data for selected columns\n",
    "print(\"MinMax Normalized Data:\")\n",
    "print(x['Power_1'].head(), x['ASE_1'].head(), x['NLI_1'].head(), x['No. Spans'].head(), x['Total Distance(m)'].head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized data with equal path distribution in train and test data :\n",
      "\n",
      "Train data shape: (8, 230)\n",
      "Test data shape: (2, 230)\n",
      "Train labels shape: (8,)\n",
      "Test labels shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Define the function to split data according to the specified pattern\n",
    "def custom_train_test_split(data, labels, samples_per_block=3000, train_samples_per_block=2500, test_samples_per_block=500):\n",
    "    # Initialize lists to hold the training and testing data and labels\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    train_labels = []\n",
    "    test_labels = []\n",
    "\n",
    "    # Determine the total number of samples in the data\n",
    "    total_samples = len(data)\n",
    "\n",
    "    # Loop over the data in blocks of the specified size (samples_per_block)\n",
    "    for start in range(0, total_samples, samples_per_block):\n",
    "        # Define the end of the current block\n",
    "        end = start + samples_per_block\n",
    "        # Define the end of the training samples within the current block\n",
    "        train_end = start + train_samples_per_block\n",
    "\n",
    "        # Adjust train_end and end if they exceed the total number of samples\n",
    "        if train_end > total_samples:\n",
    "            train_end = total_samples\n",
    "        if end > total_samples:\n",
    "            end = total_samples\n",
    "\n",
    "        # Append the training and testing data and labels for the current block to the respective lists\n",
    "        train_data.append(data[start:train_end])\n",
    "        test_data.append(data[train_end:end])\n",
    "        train_labels.append(labels[start:train_end])\n",
    "        test_labels.append(labels[train_end:end])\n",
    "\n",
    "    # Concatenate the lists into DataFrames and reset the index\n",
    "    train_data = pd.concat(train_data).reset_index(drop=True)\n",
    "    test_data = pd.concat(test_data).reset_index(drop=True)\n",
    "    train_labels = pd.concat(train_labels).reset_index(drop=True)\n",
    "    test_labels = pd.concat(test_labels).reset_index(drop=True)\n",
    "\n",
    "    # Return the training and testing data and labels\n",
    "    return train_data, test_data, train_labels, test_labels\n",
    "\n",
    "# Create a list of attribute columns excluding those that start with 'frequency'\n",
    "# attribute_columns_without_frequency = [col for col in attribute_columns if not col.startswith('frequency')]\n",
    "\n",
    "# Assuming 'x' and 'y' are your DataFrames for MinMax normalized data\n",
    "# Perform the custom train-test split on the normalized data\n",
    "# X_train, X_test, Y_train, Y_test = custom_train_test_split(x[attribute_columns_without_frequency], y[target_column])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    x[attribute_columns], \n",
    "    y[target_column], \n",
    "    test_size=0.2,  # Equivalent to test_samples_per_block / samples_per_block\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "\n",
    "# Display shapes to verify the split\n",
    "print(\"Normalized data with equal path distribution in train and test data :\\n\")\n",
    "print(\"Train data shape:\", X_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "print(\"Train labels shape:\", Y_train.shape)\n",
    "print(\"Test labels shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A.C\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax Normalized Data:\n",
      "   0         1         2         3         4         5         6         7    \\\n",
      "0  0.0  0.001137  0.193446  0.964499  0.209394  0.098348  0.037375  0.035754   \n",
      "1  0.0  0.001137  0.115766  0.000037  0.044014  0.000186  0.047866  0.072845   \n",
      "2  0.0  0.001137  0.066578  0.000037  0.044014  0.000186  0.047866  0.072845   \n",
      "3  0.0  0.001137  0.197650  0.000037  0.044014  0.000186  0.047866  0.072845   \n",
      "4  0.0  0.001137  0.117886  0.000037  0.044014  0.000186  0.047866  0.083102   \n",
      "\n",
      "        8         9    ...  502  503  504  505  506  507  508  509  510  511  \n",
      "0  0.001109  0.074251  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  0.193736  0.036502  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.612196  0.036502  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3  0.193736  0.036722  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4  0.043895  0.038042  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 512 columns]\n",
      "   0         1         2         3         4         5         6         7    \\\n",
      "0  0.0  0.001088  0.197271  0.000378  0.040751  0.000218  0.046702  0.193607   \n",
      "1  0.0  0.001088  0.117966  0.000378  0.040751  0.000218  0.046702  0.054619   \n",
      "2  0.0  0.001088  0.067750  0.000378  0.040751  0.000218  0.046702  0.054619   \n",
      "3  0.0  0.001088  0.201563  0.000378  0.040751  0.000218  0.046702  0.074575   \n",
      "4  0.0  0.001088  0.120131  0.000378  0.040751  0.000218  0.046702  0.085070   \n",
      "\n",
      "        8         9    ...  502  503  504  505  506  507  508  509  510  511  \n",
      "0  0.096971  0.035727  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  0.036749  0.002085  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.337527  0.002085  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3  0.036749  0.244051  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4  0.043534  0.036015  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 512 columns]\n",
      "Normalized data with equal path distribution in train and test data :\n",
      "\n",
      "Train data shape: (249, 512)\n",
      "Test data shape: (63, 512)\n",
      "Train labels shape: (249, 512)\n",
      "Test labels shape: (63, 512)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "# Load a dataset from Hugging Face\n",
    "dataset = load_dataset('wasifis/400')\n",
    "\n",
    "# Filter out rows with null values\n",
    "def filter_nulls(example):\n",
    "    return all(example[column] is not None for column in ['instruction', 'input', 'output'])\n",
    "\n",
    "dataset = dataset.filter(filter_nulls)\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Define the attribute columns and the target variable\n",
    "attribute_columns = ['input', 'instruction']\n",
    "target_column = 'output'\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# Tokenize the attribute and target columns\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['input'], examples['instruction'], examples['output'], truncation=True, padding='max_length')\n",
    "\n",
    "tokenized_data = df.apply(lambda row: tokenize_function(row), axis=1)\n",
    "\n",
    "# Convert tokenized data to DataFrame\n",
    "tokenized_df = pd.DataFrame(tokenized_data.tolist())\n",
    "\n",
    "# Pad the tokenized sequences\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"np\")\n",
    "padded_data = data_collator(tokenized_df.to_dict(orient='records'))\n",
    "\n",
    "# Extract the padded sequences\n",
    "input_ids = padded_data['input_ids']\n",
    "attention_mask = padded_data['attention_mask']\n",
    "labels = padded_data['labels']\n",
    "\n",
    "# MinMax Normalization\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize the padded sequences using MinMaxScaler\n",
    "x = scaler.fit_transform(input_ids)\n",
    "x = pd.DataFrame(x)\n",
    "\n",
    "# Normalize the target variable using MinMaxScaler\n",
    "y = scaler.fit_transform(labels)\n",
    "y = pd.DataFrame(y)\n",
    "\n",
    "# Display the first few rows of the normalized data for selected columns\n",
    "print(\"MinMax Normalized Data:\")\n",
    "print(x.head())\n",
    "print(y.head())\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    x, \n",
    "    y, \n",
    "    test_size=0.2,  # Equivalent to test_samples_per_block / samples_per_block\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Display shapes to verify the split\n",
    "print(\"Normalized data with equal path distribution in train and test data :\\n\")\n",
    "print(\"Train data shape:\", X_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "print(\"Train labels shape:\", Y_train.shape)\n",
    "print(\"Test labels shape:\", Y_test.shape)\n",
    "\n",
    "# Save the data to a file\n",
    "with open('data.pkl', 'wb') as f:\n",
    "    pickle.dump((X_train, Y_train, X_test, Y_test), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
